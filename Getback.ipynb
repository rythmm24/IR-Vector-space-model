{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query >> hey\n",
      "Score: filename\n",
      "Ltc.lnc score:0.023059538305076375 Doc_id:798 Title:Aries (constellation)\n",
      "Ltc.lnc score:0.014379033357917384 Doc_id:880 Title:ABBA\n",
      "Search query >> this\n",
      "Score: filename\n",
      "Ltc.lnc score:0.159352395890582 Doc_id:1332 Title:August 7\n",
      "Ltc.lnc score:0.13526729247657066 Doc_id:1110 Title:Demographics of American Samoa\n",
      "Ltc.lnc score:0.1323765622184754 Doc_id:994 Title:Arecales\n",
      "Ltc.lnc score:0.10853661145627949 Doc_id:1158 Title:Algebraic number\n",
      "Ltc.lnc score:0.10399699746209526 Doc_id:1392 Title:Dasyproctidae\n",
      "Ltc.lnc score:0.09895435240736543 Doc_id:675 Title:Affirming the consequent\n",
      "Ltc.lnc score:0.09789255946551233 Doc_id:966 Title:American shot\n",
      "Ltc.lnc score:0.09587478630628096 Doc_id:788 Title:Apiales\n",
      "Ltc.lnc score:0.09479893125876564 Doc_id:1262 Title:Argot\n",
      "Ltc.lnc score:0.09184241996377372 Doc_id:779 Title:Anthophyta\n",
      "Search query >> Anarchism\n",
      "Score: filename\n",
      "Ltc.lnc score:0.04560908291467853 Doc_id:12 Title:Anarchism\n",
      "Ltc.lnc score:0.03524632696093616 Doc_id:1023 Title:Anarcho-capitalism\n",
      "Ltc.lnc score:0.026796606417640798 Doc_id:339 Title:Ayn Rand\n"
     ]
    }
   ],
   "source": [
    "\"\"\"vsm.py implements a toy search engine to illustrate the vector\n",
    "space model for documents.\n",
    "\n",
    "It asks you to enter a search query, and then returns all documents\n",
    "matching the query, in decreasing order of cosine similarity,\n",
    "according to the vector space model.\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "import math\n",
    "import sys\n",
    "\n",
    "\n",
    "# We use a corpus of four documents.  Each document has an id, and\n",
    "# these are the keys in the following dict.  The values are the\n",
    "# corresponding filenames.\n",
    "\n",
    "\n",
    "\n",
    "# dictionary: a set to contain all terms (i.e., words) in the document\n",
    "# corpus.\n",
    "dictionary = set()\n",
    "\n",
    "# postings: a defaultdict whose keys are terms, and whose\n",
    "# corresponding values are the so-called \"postings list\" for that\n",
    "# term, i.e., the list of documents the term appears in.\n",
    "#\n",
    "# The way we implement the postings list is actually not as a Python\n",
    "# list.  Rather, it's as a dict whose keys are the document ids of\n",
    "# documents that the term appears in, with corresponding values equal\n",
    "# to the frequency with which the term occurs in the document.\n",
    "#\n",
    "# As a result, postings[term] is the postings list for term, and\n",
    "# postings[term][id] is the frequency with which term appears in\n",
    "# document id.\n",
    "\n",
    "\n",
    "#Retreiving #########################################################################\n",
    "import json\n",
    "postings_normdoc = defaultdict(dict)\n",
    "f = open('postings_normdoc.txt', 'r',encoding=\"utf-8\")\n",
    "postings_normdoc= json.loads(f.read())\n",
    "\n",
    "document_id = []\n",
    "document_title = []\n",
    "\n",
    "f = open('document_id.txt', 'r',encoding=\"utf-8\")\n",
    "document_id = json.loads(f.read())\n",
    "\n",
    "f = open('document_title.txt', 'r',encoding=\"utf-8\")\n",
    "document_title= json.loads(f.read())\n",
    "\n",
    "document_filenames = defaultdict(dict)\n",
    "f = open('document_filenames.txt', 'r',encoding=\"utf-8\")\n",
    "document_filenames= json.loads(f.read())\n",
    "\n",
    "postings = defaultdict(dict)\n",
    "f = open('postings.txt', 'r',encoding=\"utf-8\")\n",
    "postings= json.loads(f.read())\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "# The size of the corpus\n",
    "N = len(document_filenames)\n",
    "\n",
    "# document_frequency: a defaultdict whose keys are terms, with\n",
    "# corresponding values equal to the number of documents which contain\n",
    "# the key, i.e., the document frequency.\n",
    "document_frequency = defaultdict(int)\n",
    "\n",
    "# length: a defaultdict whose keys are document ids, with values equal\n",
    "# to the Euclidean length of the corresponding document vector.\n",
    "length = defaultdict(float)\n",
    "\n",
    "# The list of characters (mostly, punctuation) we want to strip out of\n",
    "# terms in the document.\n",
    "characters = \" .,!#$%^&*();:\\n\\t\\\\\\\"?!{}[]<>\"\n",
    "\n",
    "def main():\n",
    "    initialize_terms_and_postings()\n",
    "    initialize_document_frequencies()\n",
    "    initialize_lengths()\n",
    "    while True:\n",
    "        do_search()\n",
    "\n",
    "def initialize_terms_and_postings():\n",
    "    \"\"\"Reads in each document in document_filenames, splits it into a\n",
    "    list of terms (i.e., tokenizes it), adds new terms to the global\n",
    "    dictionary, and adds the document to the posting list for each\n",
    "    term, with value equal to the frequency of the term in the\n",
    "    document.\"\"\"\n",
    "    x = 0\n",
    "    \n",
    "    global dictionary, postings\n",
    "    for id in document_filenames:\n",
    "        document = document_filenames[id]\n",
    "        terms = tokenize(document)\n",
    "        unique_terms = set(terms)\n",
    "        \n",
    "        dictionary = dictionary.union(unique_terms)\n",
    "        '''for term in unique_terms:\n",
    "            postings[term][id] = terms.count(term)                      \n",
    "            postings_tfwtdoc[term][id] = 1+math.log(terms.count(term),10)\n",
    "            x = x + ((postings_tfwtdoc[term][id])**2)\n",
    "        x = math.sqrt(x)\n",
    "        \n",
    "        for term in unique_terms:\n",
    "            postings_normdoc[term][id] = postings_tfwtdoc[term][id]/x\n",
    "        \n",
    "    f = open(\"postings_normdoc.txt\",\"w\", encoding=\"utf-8\")\n",
    "    f.write(str(postings_normdoc))\n",
    "    f.close() '''\n",
    "            \n",
    "def tokenize(document):\n",
    "    \"\"\"Returns a list whose elements are the separate terms in\n",
    "    document.  Something of a hack, but for the simple documents we're\n",
    "    using, it's okay.  Note that we case-fold when we tokenize, i.e.,\n",
    "    we lowercase everything.\"\"\"\n",
    "    terms = document.lower().split()\n",
    "    return [term.strip(characters) for term in terms]\n",
    "\n",
    "def initialize_document_frequencies():\n",
    "    \"\"\"For each term in the dictionary, count the number of documents\n",
    "    it appears in, and store the value in document_frequncy[term].\"\"\"\n",
    "    global document_frequency\n",
    "    for term in dictionary:\n",
    "        document_frequency[term] = len(postings[term])\n",
    "\n",
    "def initialize_lengths():\n",
    "    \"\"\"Computes the length for each document.\"\"\"\n",
    "    global length\n",
    "    for id in document_filenames:\n",
    "        l = 0\n",
    "        for term in dictionary:\n",
    "            l += imp(term,id)**2        \n",
    "        length[id] = math.sqrt(l)\n",
    "\n",
    "\n",
    "def imp(term,id):\n",
    "    \"\"\"Returns the importance of term in document id.  If the term\n",
    "    isn't in the document, then return 0.\"\"\"\n",
    "    if id in postings[term]:\n",
    "        return postings[term][id]*inverse_document_frequency(term)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def inverse_document_frequency(term):\n",
    "    \"\"\"Returns the inverse document frequency of term.  Note that if\n",
    "    term isn't in the dictionary then it returns 0, by convention.\"\"\"\n",
    "    if term in dictionary:\n",
    "        return math.log(N/document_frequency[term],2)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def do_search():\n",
    "    \"\"\"Asks the user what they would like to search for, and returns a\n",
    "    list of relevant documents, in decreasing order of cosine\n",
    "    similarity.\"\"\"\n",
    "    query = tokenize(input(\"Search query >> \"))\n",
    "    if query == []:\n",
    "        sys.exit()\n",
    "    # find document ids containing all query terms.  Works by\n",
    "    # intersecting the posting lists for all query terms.\n",
    "    relevant_document_ids = intersection(\n",
    "            [set(postings[term].keys()) for term in query])\n",
    "    if not relevant_document_ids:\n",
    "        print(\"No documents matched all query terms.\")\n",
    "    else:\n",
    "        scores = sorted([(id,similarity(query,id))\n",
    "                         for id in relevant_document_ids],\n",
    "                        key=lambda x: x[1],\n",
    "                        reverse=True)\n",
    "        print(\"Score: filename\")\n",
    "        for (id,score) in scores[0:10]:\n",
    "            print (\"Ltc.lnc score:\"+str(score)+\" Doc_id:\"+document_id[int(id)]+\" Title:\"+document_title[int(id)])\n",
    "            \n",
    "def intersection(sets):\n",
    "    \"\"\"Returns the intersection of all sets in the list sets. Requires\n",
    "    that the list sets contains at least one element, otherwise it\n",
    "    raises an error.\"\"\"\n",
    "    return reduce(set.intersection, [s for s in sets])\n",
    "\n",
    "def similarity(query,id):\n",
    "    \"\"\"Returns the cosine similarity between query and document id.\n",
    "    Note that we don't bother dividing by the length of the query\n",
    "    vector, since this doesn't make any difference to the ordering of\n",
    "    search results.\"\"\"\n",
    "    similarity = 0.0\n",
    "    sum = 0\n",
    "    score = 0\n",
    "    for term in query:\n",
    "        if term in dictionary:\n",
    "            \n",
    "            product = postings_normdoc[term][id]\n",
    "            #sum = product + sum\n",
    "            p = inverse_document_frequency(term)*imp(term,id)\n",
    "            sum = sum + p**2\n",
    "            norm = p/math.sqrt(sum)\n",
    "            score = score + product*norm            \n",
    "            \n",
    "            similarity += inverse_document_frequency(term)*imp(term,id)\n",
    "    \n",
    "    similarity = similarity / length[id]\n",
    "    #print(score,similarity)\n",
    "    return score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
